# bot.py

import asyncio
import logging
import datetime
import time
from telegram import Bot, InputMediaPhoto, InputMediaVideo, Update, InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import Application, CommandHandler, ContextTypes, CallbackContext, filters, MessageHandler, CallbackQueryHandler
from telegram.constants import ParseMode
from config import TELEGRAM_BOT_TOKEN, TELEGRAM_CHANNEL_ID
from utils import clean_html, format_date, enhance_title, create_hashtags, prepare_article_content
from media_handler import extract_video_url, is_youtube_url, get_video_thumbnail
from stats import bot_stats
from notifications import notification_manager
from nlp_analyzer import news_analyzer
from cache_manager import cache_manager
from error_handler import handle_telegram_error, retry_on_failure, log_error, error_stats, setup_error_logging

# Enable logging
logging.basicConfig(
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s", level=logging.INFO
)
logger = logging.getLogger(__name__)

# ØªÙ… Ø­Ø°Ù Ø£ÙˆØ§Ù…Ø± Ø§Ù„ØªØ´ØºÙŠÙ„ - Ø§Ù„Ø¨ÙˆØª ÙŠØ¹Ù…Ù„ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø§Ù„Ø¢Ù†

async def stats_command(update, context):
    """Ø¹Ø±Ø¶ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¨ÙˆØª."""
    stats = bot_stats.get_summary()
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù„ÙˆØ­Ø© Ù…ÙØ§ØªÙŠØ­ Ù„Ù„ØªÙ†Ù‚Ù„ Ø¨ÙŠÙ† Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
    keyboard = [
        [InlineKeyboardButton("ğŸ“Š Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¹Ø§Ù…Ø©", callback_data="stats_general")],
        [InlineKeyboardButton("ğŸ“ˆ Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§Ù„Ø±Ø§Ø¦Ø¬Ø©", callback_data="stats_trending")],
        [InlineKeyboardButton("ğŸ˜Š ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±", callback_data="stats_sentiment")]
    ]
    reply_markup = InlineKeyboardMarkup(keyboard)
    
    # ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
    text = "ğŸ“Š *Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø¨ÙˆØª Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø¬Ø²Ø§Ø¦Ø±* ğŸ“Š\n\n"
    text += f"ğŸ“° *Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ù…Ù†Ø´ÙˆØ±Ø©:* {stats['total_articles']}\n"
    text += f"ğŸ“† *Ø¹Ø¯Ø¯ Ø£ÙŠØ§Ù… Ø§Ù„ØªØ´ØºÙŠÙ„:* {stats['days_running']}\n"
    text += f"ğŸ“ˆ *Ù…ØªÙˆØ³Ø· Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„ÙŠÙˆÙ…ÙŠ:* {stats['avg_per_day']}\n"
    text += f"ğŸ‘¥ *Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø´ØªØ±ÙƒÙŠÙ†:* {notification_manager.get_subscribers_count()}\n\n"
    
    # Ø£ÙƒØ«Ø± Ø§Ù„Ù…ØµØ§Ø¯Ø± Ù†Ø´Ø§Ø·Ø§Ù‹
    text += "ğŸ” *Ø£ÙƒØ«Ø± Ø§Ù„Ù…ØµØ§Ø¯Ø± Ù†Ø´Ø§Ø·Ø§Ù‹:*\n"
    for source, count in stats['top_sources']:
        text += f"  â€¢ {source}: {count} Ø®Ø¨Ø±\n"
    
    text += "\nğŸ“‹ *Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø­Ø³Ø¨ Ø§Ù„ØªØµÙ†ÙŠÙ:*\n"
    for category, count in stats['top_categories']:
        text += f"  â€¢ {category}: {count} Ø®Ø¨Ø±\n"
    
    text += f"\nğŸ•’ *Ø¢Ø®Ø± ØªØ­Ø¯ÙŠØ«:* {stats['last_update']}\n"
    text += "\nğŸ‘‡ Ø§Ø®ØªØ± Ù…Ù† Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø£Ø¯Ù†Ø§Ù‡ Ù„Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª"
    
    await update.message.reply_text(text, parse_mode=ParseMode.MARKDOWN, reply_markup=reply_markup)

async def stats_callback(update, context):
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¶ØºØ· Ø¹Ù„Ù‰ Ø£Ø²Ø±Ø§Ø± Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª"""
    query = update.callback_query
    await query.answer()
    
    if query.data == "stats_general":
        # Ø¥Ø¹Ø§Ø¯Ø© Ø¹Ø±Ø¶ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¹Ø§Ù…Ø©
        await stats_command(update, context)
    
    elif query.data == "stats_trending":
        # Ø¹Ø±Ø¶ Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§Ù„Ø±Ø§Ø¦Ø¬Ø©
        trending_topics = news_analyzer.get_trending_topics(15)
        
        text = "ğŸ“ˆ *Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§Ù„Ø±Ø§Ø¦Ø¬Ø© ÙÙŠ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±* ğŸ“ˆ\n\n"
        
        for i, (topic, count) in enumerate(trending_topics, 1):
            text += f"{i}. *{topic}*: {count} Ù…Ø±Ø©\n"
        
        if not trending_topics:
            text += "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø±Ø§Ø¦Ø¬Ø© Ø­ØªÙ‰ Ø§Ù„Ø¢Ù†.\n"
        
        # Ø¥Ø¶Ø§ÙØ© Ø²Ø± Ù„Ù„Ø¹ÙˆØ¯Ø©
        keyboard = [[InlineKeyboardButton("ğŸ”™ Ø§Ù„Ø¹ÙˆØ¯Ø© Ù„Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¹Ø§Ù…Ø©", callback_data="stats_general")]]
        reply_markup = InlineKeyboardMarkup(keyboard)
        
        await query.edit_message_text(text, parse_mode=ParseMode.MARKDOWN, reply_markup=reply_markup)
    
    elif query.data == "stats_sentiment":
        # Ø¹Ø±Ø¶ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
        sentiment = news_analyzer.get_sentiment_summary()
        source_sentiment = news_analyzer.get_source_sentiment()
        
        total = sentiment["positive"] + sentiment["negative"] + sentiment["neutral"]
        if total > 0:
            pos_percent = round((sentiment["positive"] / total) * 100)
            neg_percent = round((sentiment["negative"] / total) * 100)
            neu_percent = round((sentiment["neutral"] / total) * 100)
        else:
            pos_percent = neg_percent = neu_percent = 0
        
        text = "ğŸ˜Š *ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± ÙÙŠ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±* ğŸ˜Š\n\n"
        text += f"*Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø¹Ø§Ù… Ù„Ù„Ù…Ø´Ø§Ø¹Ø±:*\n"
        text += f"ğŸ˜ƒ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ: {sentiment['positive']} ({pos_percent}%)\n"
        text += f"ğŸ˜¡ Ø³Ù„Ø¨ÙŠ: {sentiment['negative']} ({neg_percent}%)\n"
        text += f"ğŸ˜ Ù…Ø­Ø§ÙŠØ¯: {sentiment['neutral']} ({neu_percent}%)\n\n"
        
        text += "*ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø­Ø³Ø¨ Ø§Ù„Ù…ØµØ¯Ø±:*\n"
        for source, data in list(source_sentiment.items())[:5]:  # Ø£Ø®Ø° Ø£ÙˆÙ„ 5 Ù…ØµØ§Ø¯Ø± ÙÙ‚Ø·
            total_source = data["positive"] + data["negative"] + data["neutral"]
            if total_source > 0:
                pos_percent = round((data["positive"] / total_source) * 100)
                neg_percent = round((data["negative"] / total_source) * 100)
                neu_percent = round((data["neutral"] / total_source) * 100)
                
                text += f"*{source}*: ğŸ˜ƒ {pos_percent}% | ğŸ˜¡ {neg_percent}% | ğŸ˜ {neu_percent}%\n"
        
        # Ø¥Ø¶Ø§ÙØ© Ø²Ø± Ù„Ù„Ø¹ÙˆØ¯Ø©
        keyboard = [[InlineKeyboardButton("ğŸ”™ Ø§Ù„Ø¹ÙˆØ¯Ø© Ù„Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¹Ø§Ù…Ø©", callback_data="stats_general")]]
        reply_markup = InlineKeyboardMarkup(keyboard)
        
        await query.edit_message_text(text, parse_mode=ParseMode.MARKDOWN, reply_markup=reply_markup)

async def start_command(update, context):
    """Ø¨Ø¯Ø¡ Ø§Ù„Ø¨ÙˆØª ÙˆØ§Ù„Ø§Ø´ØªØ±Ø§Ùƒ ÙÙŠ Ø§Ù„Ø¥Ø´Ø¹Ø§Ø±Ø§Øª."""
    user = update.effective_user
    added = notification_manager.add_user(user.id)
    
    if added:
        await update.message.reply_text(
            f"ğŸ‘‹ Ø£Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ ÙŠØ§ {user.first_name}!\n\n"
            "Ø£Ù†Ø§ Ø¨ÙˆØª Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø¬Ø²Ø§Ø¦Ø±ØŒ Ø³Ø£Ø¨Ù‚ÙŠÙƒ Ø¹Ù„Ù‰ Ø§Ø·Ù„Ø§Ø¹ Ø¨Ø¢Ø®Ø± Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† Ù…ØµØ§Ø¯Ø± Ù…ÙˆØ«ÙˆÙ‚Ø©.\n\n"
            "ØªÙ… Ø§Ø´ØªØ±Ø§ÙƒÙƒ Ø¨Ù†Ø¬Ø§Ø­ ÙÙŠ Ù†Ø¸Ø§Ù… Ø§Ù„Ø¥Ø´Ø¹Ø§Ø±Ø§Øª Ø§Ù„ÙÙˆØ±ÙŠØ©. âœ…\n"
            "Ù„Ø¥Ù„ØºØ§Ø¡ Ø§Ù„Ø§Ø´ØªØ±Ø§Ùƒ ÙÙŠ Ø£ÙŠ ÙˆÙ‚ØªØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± /unsubscribe.",
            parse_mode=ParseMode.MARKDOWN
        )
    else:
        await update.message.reply_text(
            f"Ø£Ù‡Ù„Ø§Ù‹ Ø¨Ø¹ÙˆØ¯ØªÙƒ ÙŠØ§ {user.first_name}! Ø£Ù†Øª Ù…Ø´ØªØ±Ùƒ Ø¨Ø§Ù„ÙØ¹Ù„. ğŸ‘\n\n"
            "Ù„Ø§ Ø¯Ø§Ø¹ÙŠ Ù„Ù„Ø§Ø´ØªØ±Ø§Ùƒ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰. Ø³ØªØµÙ„Ùƒ Ø¢Ø®Ø± Ø§Ù„Ø£Ø®Ø¨Ø§Ø± ÙÙˆØ± ÙˆØ±ÙˆØ¯Ù‡Ø§."
        )

async def subscribe_command(update, context):
    """Ø§Ù„Ø§Ø´ØªØ±Ø§Ùƒ ÙÙŠ Ù†Ø¸Ø§Ù… Ø§Ù„Ø¥Ø´Ø¹Ø§Ø±Ø§Øª."""
    user = update.effective_user
    added = notification_manager.add_user(user.id)
    
    if added:
        await update.message.reply_text("ØªÙ… Ø§Ø´ØªØ±Ø§ÙƒÙƒ ÙÙŠ Ù†Ø¸Ø§Ù… Ø§Ù„Ø¥Ø´Ø¹Ø§Ø±Ø§Øª Ø¨Ù†Ø¬Ø§Ø­. âœ…")
    else:
        await update.message.reply_text("Ø£Ù†Øª Ù…Ø´ØªØ±Ùƒ Ø¨Ø§Ù„ÙØ¹Ù„. ğŸ‘")

async def unsubscribe_command(update, context):
    """Ø¥Ù„ØºØ§Ø¡ Ø§Ù„Ø§Ø´ØªØ±Ø§Ùƒ Ù…Ù† Ù†Ø¸Ø§Ù… Ø§Ù„Ø¥Ø´Ø¹Ø§Ø±Ø§Øª."""
    user = update.effective_user
    removed = notification_manager.remove_user(user.id)
    
    if removed:
        await update.message.reply_text("ØªÙ… Ø¥Ù„ØºØ§Ø¡ Ø§Ø´ØªØ±Ø§ÙƒÙƒ Ù…Ù† Ù†Ø¸Ø§Ù… Ø§Ù„Ø¥Ø´Ø¹Ø§Ø±Ø§Øª Ø¨Ù†Ø¬Ø§Ø­. âœ…\n\nÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø§Ø´ØªØ±Ø§Ùƒ Ù…Ø¬Ø¯Ø¯Ø§Ù‹ ÙÙŠ Ø£ÙŠ ÙˆÙ‚Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ù…Ø± /start")
    else:
        await update.message.reply_text("Ø£Ù†Øª ØºÙŠØ± Ù…Ø´ØªØ±Ùƒ ÙÙŠ Ù†Ø¸Ø§Ù… Ø§Ù„Ø¥Ø´Ø¹Ø§Ø±Ø§Øª. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± /start Ù„Ù„Ø§Ø´ØªØ±Ø§Ùƒ.")

async def trends_command(update, context):
    """Ø¹Ø±Ø¶ Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§Ù„Ø±Ø§Ø¦Ø¬Ø©."""
    trending_topics = news_analyzer.get_trending_topics(10)
    
    text = "ğŸ“ˆ *Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§Ù„Ø±Ø§Ø¦Ø¬Ø© ÙÙŠ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±* ğŸ“ˆ\n\n"
    
    for i, (topic, count) in enumerate(trending_topics, 1):
        text += f"{i}. *{topic}*: {count} Ù…Ø±Ø©\n"
    
    if not trending_topics:
        text += "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø±Ø§Ø¦Ø¬Ø© Ø­ØªÙ‰ Ø§Ù„Ø¢Ù†.\n"
    
    await update.message.reply_text(text, parse_mode=ParseMode.MARKDOWN)

def main() -> None:
    """Run the bot."""
    # Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
    setup_error_logging()
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
    application = Application.builder().token(TELEGRAM_BOT_TOKEN).build()

    # Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ø§Ù„Ø¬Ø§Øª Ø§Ù„Ø£ÙˆØ§Ù…Ø±
    application.add_handler(CommandHandler("start", start_command))
    application.add_handler(CommandHandler("stats", stats_command))
    application.add_handler(CommandHandler("subscribe", subscribe_command))
    application.add_handler(CommandHandler("unsubscribe", unsubscribe_command))
    application.add_handler(CommandHandler("trends", trends_command))
    
    # Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ø§Ù„Ø¬ Ù„Ù„Ø£Ø²Ø±Ø§Ø± Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ©
    application.add_handler(CallbackQueryHandler(stats_callback, pattern='^stats_'))
    application.add_handler(CallbackQueryHandler(read_more_callback, pattern='^read_more:'))


    # Ø¬Ø¯ÙˆÙ„Ø© Ù…Ù‡Ù…Ø© Ø¬Ù…Ø¹ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± ÙƒÙ„ 30 Ø«Ø§Ù†ÙŠØ©
    job_queue = application.job_queue
    job_queue.run_repeating(fetch_and_send_news, interval=30, first=10) # 30 seconds
    
    # Ø¬Ø¯ÙˆÙ„Ø© ØªÙ†Ø¸ÙŠÙ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª ÙƒÙ„ Ø³Ø§Ø¹Ø©
    job_queue.run_repeating(lambda context: cache_manager.clear_old_cache(), interval=3600, first=3600)

    logger.info("ğŸš€ Ø¨ÙˆØª Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø¬Ø²Ø§Ø¦Ø± Ø¨Ø¯Ø£ Ø§Ù„Ø¹Ù…Ù„ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹...")
    logger.info("ğŸ“° Ø³ÙŠØªÙ… Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± ÙÙˆØ± ØµØ¯ÙˆØ±Ù‡Ø§ ÙˆÙ†Ø´Ø±Ù‡Ø§ ÙƒÙ„ 30 Ø«Ø§Ù†ÙŠØ©")
    logger.info("ğŸ—‚ï¸ Ø³ÙŠØªÙ… ØªÙ†Ø¸ÙŠÙ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª ÙƒÙ„ Ø³Ø§Ø¹Ø©")
    
    # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¨ÙˆØª Ø­ØªÙ‰ ÙŠØ¶ØºØ· Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ctrl-C
    application.run_polling()

def diversify_articles_by_source(articles):
    """ØªÙ†ÙˆÙŠØ¹ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ù„ØªØ¬Ù†Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø± Ù…Ù† Ù†ÙØ³ Ø§Ù„Ù…ØµØ¯Ø± Ù…ØªØªØ§Ù„ÙŠØ§Ù‹."""
    if not articles:
        return []
    
    # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ø­Ø³Ø¨ Ø§Ù„Ù…ØµØ¯Ø±
    articles_by_source = {}
    for article in articles:
        source_name = article.source
        if source_name not in articles_by_source:
            articles_by_source[source_name] = []
        articles_by_source[source_name].append(article)
    
    # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø­Ø³Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª (Ø§Ù„Ø£Ù‚Ù„ Ø£ÙˆÙ„Ø§Ù‹ Ù„Ø¶Ù…Ø§Ù† Ø§Ù„ØªÙ†ÙˆÙŠØ¹)
    sorted_sources = sorted(articles_by_source.keys(), 
                           key=lambda x: len(articles_by_source[x]))
    
    # ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ø¨Ø§Ù„ØªÙ†Ø§ÙˆØ¨ Ø¨ÙŠÙ† Ø§Ù„Ù…ØµØ§Ø¯Ø±
    diversified = []
    max_articles = max(len(articles_by_source[source]) for source in sorted_sources)
    
    for i in range(max_articles):
        for source in sorted_sources:
            if i < len(articles_by_source[source]):
                diversified.append(articles_by_source[source][i])
    
    return diversified

async def process_source(source, session):
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØµØ¯Ø± Ø£Ø®Ø¨Ø§Ø± ÙˆØ§Ø­Ø¯ Ù„Ø¬Ù„Ø¨ ÙˆØ­ÙØ¸ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©."""
    from rss_parser import parse_rss_feed
    from web_scraper import scrape_website, scrape_article_content
    from database import Article
    from nlp_analyzer import news_analyzer
    from classifier import classify_article
    from media_handler import extract_video_url

    new_articles = []
    try:
        logger.info(f"Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØµØ¯Ø±: {source['name']} - Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ©: {source.get('priority', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯Ø©')}")
        
        articles_data = []
        if source['type'] == 'rss':
            articles_data = parse_rss_feed(source['url'])
        elif source['type'] == 'scrape':
            articles_data = scrape_website(source)

        # ÙÙ„ØªØ±Ø© Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ø­Ø³Ø¨ Ø§Ù„ØªØ§Ø±ÙŠØ® (Ø¢Ø®Ø± 24 Ø³Ø§Ø¹Ø© ÙÙ‚Ø·)
        twenty_four_hours_ago = datetime.datetime.utcnow() - datetime.timedelta(hours=24)
        
        for article_data in articles_data:
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ù‚Ø§Ù„ Ù…Ø³Ø¨Ù‚Ø§Ù‹
            exists = session.query(Article).filter_by(link=article_data['link']).first()
            if not exists:
                # ÙÙ„ØªØ±Ø© Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© (Ø£ÙƒØ«Ø± Ù…Ù† 24 Ø³Ø§Ø¹Ø©)
                if article_data['published_date'] < twenty_four_hours_ago:
                    logger.debug(f"ØªØ¬Ø§Ù‡Ù„ Ù…Ù‚Ø§Ù„ Ù‚Ø¯ÙŠÙ…: {article_data['title']}")
                    continue
                    
                logger.info(f"Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù‚Ø§Ù„ Ø¬Ø¯ÙŠØ¯: {article_data['title']}")

                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙƒØ§Ù…Ù„
                full_content = scrape_article_content(article_data['link'], source.get('scraping_config'))
                summary = full_content if full_content else article_data['summary']
                
                # ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ù‚Ø§Ù„
                category = classify_article(article_data['title'], summary)
                
                # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…ØµØ¯Ø± Ù„Ù‡ ÙØ¦Ø© Ù…Ø­Ø¯Ø¯Ø©ØŒ Ø§Ø³ØªØ®Ø¯Ù…Ù‡Ø§
                if 'category' in source:
                    category = source['category']
                
                # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
                sentiment_score = news_analyzer.analyze_sentiment(article_data['title'] + " " + summary)

                # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù‚Ø§Ù„ Ø¬Ø¯ÙŠØ¯
                new_article = Article(
                    title=article_data['title'],
                    sentiment_score=sentiment_score,
                    link=article_data['link'],
                    source=article_data['source_name'],
                    published_date=article_data['published_date'],
                    category=category,
                    image_url=article_data.get('image_url', ''),
                    summary=summary
                )
                session.add(new_article)
                new_articles.append(new_article)

        if new_articles:
            session.flush() # Flush to get IDs for new articles
            for article in new_articles:
                session.refresh(article) # Refresh to get the ID
            logger.info(f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(new_articles)} Ù…Ù‚Ø§Ù„ Ø¬Ø¯ÙŠØ¯ Ù…Ù† {source['name']}")
        else:
            logger.debug(f"Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù‚Ø§Ù„Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© Ù…Ù† {source['name']}")
            
    except Exception as e:
        log_error(e, f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØµØ¯Ø±: {source['name']}")
        error_stats.record_error(e, f"source_{source['name']}")
        logger.error(f"ÙØ´Ù„ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØµØ¯Ø± {source['name']}: {str(e)}")
    
    return new_articles

async def fetch_and_send_news(context):
    """Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† Ø§Ù„Ù…ØµØ§Ø¯Ø± ÙˆØ¥Ø±Ø³Ø§Ù„Ù‡Ø§ Ù„Ù„ØªÙ„ÙŠØ¬Ø±Ø§Ù… Ù…Ø¹ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ§Øª ÙˆØ§Ù„ØªÙ†ÙˆÙŠØ¹."""
    from sources import NEWS_SOURCES, get_source_by_name
    from database import get_db_session, get_random_unsent_high_sentiment_article

    logger.info("ğŸ”„ Ø¨Ø¯Ø¡ Ø¯ÙˆØ±Ø© Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©...")
    session = get_db_session()
    
    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø¨Ø´ÙƒÙ„ Ù…ØªØ²Ø§Ù…Ù†
    tasks = [process_source(source, session) for source in NEWS_SOURCES]
    results = await asyncio.gather(*tasks)
    
    # ØªØ¬Ù…ÙŠØ¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
    all_new_articles = [article for sublist in results for article in sublist]
    
    # ÙÙ„ØªØ±Ø© Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù„Ù„Ù€ 24 Ø³Ø§Ø¹Ø© Ø§Ù„Ù…Ø§Ø¶ÙŠØ© ÙÙ‚Ø·
    twenty_four_hours_ago = datetime.datetime.utcnow() - datetime.timedelta(hours=24)
    recent_articles = [article for article in all_new_articles 
                      if article.published_date >= twenty_four_hours_ago]
    
    if recent_articles:
        session.commit()
        logger.info(f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(recent_articles)} Ù…Ù‚Ø§Ù„ Ø¬Ø¯ÙŠØ¯ Ø®Ù„Ø§Ù„ Ø¢Ø®Ø± 24 Ø³Ø§Ø¹Ø©")
    else:
        session.rollback()
        logger.info("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£Ø®Ø¨Ø§Ø± Ø¬Ø¯ÙŠØ¯Ø© Ø®Ù„Ø§Ù„ Ø¢Ø®Ø± 24 Ø³Ø§Ø¹Ø©")

    # ØªØ±ØªÙŠØ¨ ÙˆØªÙ†ÙˆÙŠØ¹ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø­Ø³Ø¨ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ©
    if recent_articles:
        # ØªØµÙ†ÙŠÙ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø­Ø³Ø¨ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ©
        urgent_news = []
        local_news = []  # Ø£Ø®Ø¨Ø§Ø± Ù…Ø­Ù„ÙŠØ© Ø¬Ø²Ø§Ø¦Ø±ÙŠØ©
        official_news = []
        economic_news = []
        sports_news = []
        other_news = []

        urgent_keywords = ['Ø¹Ø§Ø¬Ù„', 'Ø·Ø§Ø±Ø¦', 'Ø§Ù†ÙØ¬Ø§Ø±', 'Ø­Ø§Ø¯Ø«', 'ÙˆÙØ§Ø©', 'Ø§Ø³ØªÙ‚Ø§Ù„Ø©', 'Ø¥Ø¹Ù„Ø§Ù† Ù‡Ø§Ù…', 'Ù‚Ø±Ø§Ø± Ø¹Ø§Ø¬Ù„']
        
        for article in recent_articles:
            title_lower = article.title.lower()
            source_country = get_source_by_name(article.source).get('country', 'Unknown')

            if any(keyword in title_lower for keyword in urgent_keywords):
                urgent_news.append(article)
            elif source_country == 'DZ':
                local_news.append(article)
            elif article.category == 'official':
                official_news.append(article)
            elif article.category in ['economic', 'Ø§Ù‚ØªØµØ§Ø¯']:
                economic_news.append(article)
            elif article.category in ['sports', 'Ø±ÙŠØ§Ø¶Ø©']:
                sports_news.append(article)
            else:
                other_news.append(article)

        # ØªØ±ØªÙŠØ¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø­Ø³Ø¨ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ù…Ø¹ Ø§Ù„ØªÙ†ÙˆÙŠØ¹
        prioritized_articles = []
        prioritized_articles.extend(urgent_news)
        prioritized_articles.extend(diversify_articles_by_source(local_news))
        
        remaining_articles = official_news + economic_news + other_news + sports_news
        diversified_articles = diversify_articles_by_source(remaining_articles)
        prioritized_articles.extend(diversified_articles)

        logger.info(f"Ø¥Ø±Ø³Ø§Ù„ {len(prioritized_articles)} Ù…Ù‚Ø§Ù„ Ù…Ø¹ ØªØ±ØªÙŠØ¨ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© ÙˆØ§Ù„ØªÙ†ÙˆÙŠØ¹")
        logger.info(f"Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø¹Ø§Ø¬Ù„Ø©: {len(urgent_news)}, Ø§Ù„Ù…Ø­Ù„ÙŠØ©: {len(local_news)}, Ø§Ù„Ø±Ø³Ù…ÙŠØ©: {len(official_news)}, Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ÙŠØ©: {len(economic_news)}, Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ©: {len(sports_news)}, Ø£Ø®Ø±Ù‰: {len(other_news)}")
        
        # Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ù…Ø¹ ÙØªØ±Ø§Øª Ø²Ù…Ù†ÙŠØ© Ù…Ù†Ø§Ø³Ø¨Ø©
        # Ø£ÙˆÙ„Ø§Ù‹: Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø¹Ø§Ø¬Ù„Ø© Ø§Ù„Ù…Ø­Ù„ÙŠØ© ÙÙ‚Ø· ÙƒÙ„ 30 Ø«Ø§Ù†ÙŠØ©
        urgent_local_news = [a for a in urgent_news if a in local_news]
        other_news = [a for a in prioritized_articles if a not in urgent_local_news]

        for i, article in enumerate(urgent_local_news):
            try:
                await send_article_to_telegram(context.bot, article)
                article.sent_to_telegram = True
                session.commit()
                bot_stats.add_article(article.source, article.category)
                await notification_manager.notify_users(article, article.category)
                logger.info("Ø§Ù†ØªØ¸Ø§Ø± 30 Ø«Ø§Ù†ÙŠØ© (Ø®Ø¨Ø± Ø¹Ø§Ø¬Ù„ Ù…Ø­Ù„ÙŠ) Ù‚Ø¨Ù„ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø®Ø¨Ø± Ø§Ù„ØªØ§Ù„ÙŠ")
                await asyncio.sleep(30)
            except Exception as e:
                log_error(e, f"ÙØ´Ù„ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù‚Ø§Ù„: {article.title}")
                error_stats.record_error(e, "send_article_loop")
                session.rollback()

        # Ø«Ù…: Ø¥Ø±Ø³Ø§Ù„ Ø¨Ù‚ÙŠØ© Ø§Ù„Ø£Ø®Ø¨Ø§Ø± (Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© ÙˆØ§Ù„Ù…Ø­Ù„ÙŠØ© ØºÙŠØ± Ø§Ù„Ø¹Ø§Ø¬Ù„Ø©) Ø¨ÙØ§ØµÙ„ 30 Ø«Ø§Ù†ÙŠØ© Ø¨ÙŠÙ† ÙƒÙ„ Ø®Ø¨Ø±
        for i, article in enumerate(other_news):
            try:
                await send_article_to_telegram(context.bot, article)
                article.sent_to_telegram = True
                session.commit()
                bot_stats.add_article(article.source, article.category)
                await notification_manager.notify_users(article, article.category)
                logger.info("Ø§Ù†ØªØ¸Ø§Ø± 30 Ø«Ø§Ù†ÙŠØ© Ù‚Ø¨Ù„ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø®Ø¨Ø± Ø§Ù„ØªØ§Ù„ÙŠ")
                await asyncio.sleep(30)
            except Exception as e:
                log_error(e, f"ÙØ´Ù„ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù‚Ø§Ù„: {article.title}")
                error_stats.record_error(e, "send_article_loop")
                session.rollback()
    logger.info(f"Ø§Ù†ØªÙ‡Ø§Ø¡ Ø¯ÙˆØ±Ø© Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±. ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(recent_articles) if recent_articles else 0} Ù…Ù‚Ø§Ù„ Ø¬Ø¯ÙŠØ¯")
    
    # ØªØ³Ø¬ÙŠÙ„ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
    error_summary = error_stats.get_stats()
    if error_summary['total_errors'] > 0:
        logger.warning(f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø¯ÙˆØ±Ø©: {error_summary['total_errors']}")

    # Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£Ø®Ø¨Ø§Ø± Ø¬Ø¯ÙŠØ¯Ø©ØŒ Ø¬Ø±Ø¨ Ø¥Ø±Ø³Ø§Ù„ Ù…Ù‚Ø§Ù„ Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ø¨Ù…Ø´Ø§Ø¹Ø± Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ©
    if not recent_articles:
        logger.info("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£Ø®Ø¨Ø§Ø± Ø¬Ø¯ÙŠØ¯Ø©. Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ù‚Ø§Ù„Ø§Øª ØºÙŠØ± Ù…Ø±Ø³Ù„Ø© Ø¨Ù…Ø´Ø§Ø¹Ø± Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ©...")
        session = get_db_session()
        random_article = get_random_unsent_high_sentiment_article(session)
        if random_article:
            logger.info(f"Ø¥Ø±Ø³Ø§Ù„ Ù…Ù‚Ø§Ù„ Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ø¨Ù…Ø´Ø§Ø¹Ø± Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ©: {random_article.title}")
            await send_article_to_telegram(context.bot, random_article)
            random_article.sent_to_telegram = True # Mark as sent
            session.commit()
        else:
            logger.info("No high-sentiment unsent articles found in the last 24 hours.")
        session.close()

async def read_more_callback(update: Update, context: CallbackContext):
    """Callback function for the 'Read More' button."""
    query = update.callback_query
    article_id = query.data.split(':')[1]
    try:
        article_id_int = int(article_id)
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­ÙˆÙŠÙ„ article_id Ø¥Ù„Ù‰ int: {article_id} - {e}")
        await query.answer(text="Ø®Ø·Ø£ Ø¯Ø§Ø®Ù„ÙŠ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ø²ÙŠØ¯.", show_alert=True)
        return

    from database import get_db_session, Article
    session = get_db_session()
    article = session.query(Article).filter_by(id=article_id_int).first()
    logger.info(f"Ø²Ø± Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ø²ÙŠØ¯: article_id={article_id}, Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {bool(article)}")
    session.close()

    if article:
        await query.answer()
        # Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙƒØ§Ù…Ù„ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø°ÙŠ Ø¶ØºØ· Ø¹Ù„Ù‰ Ø§Ù„Ø²Ø±
        await query.message.reply_text(
            text=f"<b>{article.title}</b>\n\n{article.summary}",
            parse_mode=ParseMode.HTML,
            disable_web_page_preview=True
        )
    else:
        await query.answer(text="Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø®Ø¨Ø±.", show_alert=True)

async def send_article_to_telegram(bot, article):
    """ØªÙ†Ø³ÙŠÙ‚ ÙˆØ¥Ø±Ø³Ø§Ù„ Ù…Ù‚Ø§Ù„ ÙˆØ§Ø­Ø¯ Ø¥Ù„Ù‰ Ù‚Ù†Ø§Ø© Ø§Ù„ØªÙ„ÙŠØ¬Ø±Ø§Ù… Ù…Ø¹ Ø¹Ø±Ø¶ Ù…Ø­Ø³Ù† Ù„Ù„Ù…Ø­ØªÙˆÙ‰."""
    from classifier import classify_article, get_emoji_for_category

    category = classify_article(article.title, article.summary)
    category_emoji = get_emoji_for_category(category)

    # ØªÙ†Ø¸ÙŠÙ ÙˆØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†ØµÙˆØµ
    title = enhance_title(article.title)
    
    # ØªØ­Ø³ÙŠÙ† Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ - Ø¹Ø±Ø¶ Ù…Ù„Ø®Øµ Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙƒØ§Ù…Ù„
    from sources import get_source_by_name
    source_config = get_source_by_name(article.source)
    unwanted_phrases = source_config.get('scraping_config', {}).get('unwanted_phrases', [])
    content = prepare_article_content(article.summary, article.title, unwanted_phrases)

    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù‡Ø§Ø´ØªØ§Ø¬Ø§Øª
    hashtags = create_hashtags(article.title, content, article.source, category)

    # ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…Ø­Ø³Ù†
    message = f"{category_emoji} <b>{title}</b>\n\n"
    
    # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙÙ‚Ø· Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ÙÙŠØ¯Ø§Ù‹ ÙˆÙ…Ø®ØªØµØ±Ø§Ù‹
    if content and len(content.strip()) > 20:
        message += f"{content}\n\n"
    
    message += f"<a href='{article.link}'>ğŸ”— Ø§Ù„Ù…ØµØ¯Ø±: {article.source}</a>\n"
    message += f"ğŸ“… {format_date(article.published_date)}\n\n"
    message += f"{hashtags}"

    text = message

    try:
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù…Ø­ØªÙˆÙ‰ Ù…Ø±Ø¦ÙŠ (ÙÙŠØ¯ÙŠÙˆ Ø£Ùˆ ØµÙˆØ±Ø©)
        video_url = extract_video_url(article.link)
        has_media = bool(article.image_url or video_url)
        
        # Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ ÙÙŠØ¯ÙŠÙˆ Ù…Ø¨Ø§Ø´Ø± (Ù„ÙŠØ³ ÙŠÙˆØªÙŠÙˆØ¨)ØŒ Ù†Ø±Ø³Ù„Ù‡ Ù…Ø¹ Ø§Ù„Ù†Øµ
        if video_url and not is_youtube_url(video_url):
            try:
                await bot.send_video(
                    chat_id=TELEGRAM_CHANNEL_ID,
                    video=video_url,
                    caption=text,
                    parse_mode=ParseMode.HTML
                )
                logger.info(f"ØªÙ… Ø¥Ø±Ø³Ø§Ù„ Ù…Ù‚Ø§Ù„ Ù…Ø¹ ÙÙŠØ¯ÙŠÙˆ: {title}")
                return
            except Exception as e:
                logger.error(f"ÙØ´Ù„ ÙÙŠ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ù„Ù„Ù…Ù‚Ø§Ù„ {title}: {e}")
                # ÙÙŠ Ø­Ø§Ù„Ø© ÙØ´Ù„ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆØŒ Ù†Ø³ØªÙ…Ø± Ù„Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø®Ø¨Ø± Ù…Ø¹ Ø§Ù„ØµÙˆØ±Ø© Ø£Ùˆ Ø¨Ø¯ÙˆÙ†Ù‡Ø§
        
        # Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ ÙÙŠØ¯ÙŠÙˆ ÙŠÙˆØªÙŠÙˆØ¨ØŒ Ù†Ø¶ÙŠÙ Ø±Ø§Ø¨Ø·Ù‡ ÙÙŠ Ø§Ù„Ù†Øµ
        if video_url and is_youtube_url(video_url):
            text += f"\n\nğŸ¬ <a href='{video_url}'>Ø´Ø§Ù‡Ø¯ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ</a>"
        
        # Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ ØµÙˆØ±Ø©ØŒ Ù†Ø±Ø³Ù„Ù‡Ø§ Ù…Ø¹ Ø§Ù„Ø®Ø¨Ø±
        image_url = article.image_url
        
        # Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù‡Ù†Ø§Ùƒ ØµÙˆØ±Ø© ÙˆÙ„ÙƒÙ† Ù‡Ù†Ø§Ùƒ ÙÙŠØ¯ÙŠÙˆ ÙŠÙˆØªÙŠÙˆØ¨ØŒ Ù†Ø³ØªØ®Ø¯Ù… ØµÙˆØ±Ø© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø§Ù„Ù…ØµØºØ±Ø©
        if not image_url and video_url and is_youtube_url(video_url):
            image_url = get_video_thumbnail(video_url)
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ù„Ù„ØµÙˆØ±
        if image_url:
            cached_image = cache_manager.get_cached_image(image_url)
            image_url = cached_image
        
        await _send_telegram_message(bot, image_url, text, title, category, article.id, parse_mode=ParseMode.HTML)
        
    except Exception as e:
        log_error(e, f"send_article_to_telegram: {title}")
        error_stats.record_error(e, "send_article_to_telegram")
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø¥Ø±Ø³Ø§Ù„ Ø¨Ø¯ÙˆÙ† ØµÙˆØ±Ø© ÙÙŠ Ø­Ø§Ù„Ø© ÙØ´Ù„ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„ØµÙˆØ±Ø©
        try:
            await _send_telegram_message(bot, None, text, title, category, article.id)
        except Exception as e2:
            log_error(e2, f"send_article_to_telegram_fallback: {title}")
            error_stats.record_error(e2, "send_article_to_telegram_fallback")

@handle_telegram_error
@retry_on_failure(max_retries=2)
async def _send_telegram_message(bot, image_url, text, title, category, article_id, parse_mode=ParseMode.HTML):
    """Ø¥Ø±Ø³Ø§Ù„ Ø±Ø³Ø§Ù„Ø© Ø¥Ù„Ù‰ Ø§Ù„ØªÙ„ÙŠØ¬Ø±Ø§Ù… Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡"""
    keyboard = [[InlineKeyboardButton("ğŸ“° Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ø²ÙŠØ¯", callback_data=f'read_more:{article_id}')]]
    reply_markup = InlineKeyboardMarkup(keyboard)

    if image_url:
        # Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„ØµÙˆØ±Ø© Ù…Ø¹ Ø§Ù„Ù†Øµ
        await bot.send_photo(
            chat_id=TELEGRAM_CHANNEL_ID,
            photo=image_url,
            caption=text,
            parse_mode=parse_mode,
            reply_markup=reply_markup
        )
        logger.info(f"ØªÙ… Ø¥Ø±Ø³Ø§Ù„ Ù…Ù‚Ø§Ù„ Ù…Ø¹ ØµÙˆØ±Ø©: {title} (Ø§Ù„ÙØ¦Ø©: {category})")
    else:
        # Ø¥Ø±Ø³Ø§Ù„ Ù†Øµ ÙÙ‚Ø· Ù…Ø¹ Ù…Ø¹Ø§ÙŠÙ†Ø© Ø§Ù„Ø±Ø§Ø¨Ø·
        await bot.send_message(
            chat_id=TELEGRAM_CHANNEL_ID,
            text=text,
            parse_mode=parse_mode,
            disable_web_page_preview=False,  # Ø¥Ø¸Ù‡Ø§Ø± Ù…Ø¹Ø§ÙŠÙ†Ø© Ø§Ù„Ø±Ø§Ø¨Ø·
            reply_markup=reply_markup
        )
        logger.info(f"ØªÙ… Ø¥Ø±Ø³Ø§Ù„ Ù…Ù‚Ø§Ù„ Ù†ØµÙŠ: {title} (Ø§Ù„ÙØ¦Ø©: {category})")


if __name__ == "__main__":
    asyncio.run(main())